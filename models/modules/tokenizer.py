import torch
import torch.nn as nn
from einops import rearrange
from models.modules.dcae import DCAE, dc_ae_f32c32


def poses_to_indices(poses, x_divisions=128, y_divisions=128):
    x_min = 0
    x_range = 8
    y_range = 1
    y_step = y_range / y_divisions
    y_min = -0.5 + y_step / 2
    x, y = poses[..., 0], poses[..., 1]
    idx_x = (
        (x * x_divisions / x_range).clip(0, x_divisions).to(torch.float32).unsqueeze(dim=2)
    )  ##normalize [0, 1] then multiple x_divisions or y_divisions
    idx_y = (
        ((y - y_min) * y_divisions / y_range)
        .clip(0, y_divisions)
        .to(torch.float32)
        .unsqueeze(dim=2)
    )
    return torch.cat([idx_x, idx_y], dim=2)


def indices_to_pose(idx_x, idx_y, x_division=128, y_divisions=128):
    x_min = 0
    x_range = 8
    y_range = 1
    y_step = y_range / y_divisions
    y_min = -0.5 + y_step / 2
    x_step = x_range / x_division

    x = idx_x * x_step + x_step / 2
    y = idx_y * y_step + y_min + y_step / 2
    return x, y


def yaws_to_indices(yaws, division=512):
    yaw_range = 16
    yaw_step = yaw_range / division
    yaw_min = -yaw_range / 2.0 + yaw_step / 2.0
    idx_yaw = ((yaws - yaw_min) * division / yaw_range).clip(0, division).to(torch.float32)
    return idx_yaw


def indices_to_yaws(idx_yaw, division=512):
    yaw_range = 16
    yaw_step = yaw_range / division
    yaw_min = -yaw_range / 2.0 + yaw_step / 2.0
    yaw = idx_yaw * yaw_step + yaw_min + yaw_step / 2.0
    return yaw


def patchify(x, patch_size):
    bsz, c, h, w = x.shape
    p = patch_size
    h_, w_ = h // p, w // p

    x = x.reshape(bsz, c, h_, p, w_, p)
    x = torch.einsum("nchpwq->nhwcpq", x)
    x = x.reshape(bsz, h_ * w_, c * p**2)
    return x  # [b, L, c]


def unpatchify(x, patch_size, vae_embed_dim):
    bsz, h_, w_, _ = x.shape
    p = patch_size
    c = vae_embed_dim

    x = x.reshape(bsz, h_, w_, c, p, p)
    x = torch.einsum("nhwcpq->nchpwq", x)
    x = x.reshape(bsz, c, h_ * p, w_ * p)
    return x


class VAETokenizer(nn.Module):
    def __init__(self, args, local_rank):
        super().__init__()
        self.args = args
        self.vae = DCAE(
            dc_ae_f32c32(
                "dc-ae-f32c32-mix-1.0",
                pretrained_path=args.vae_ckpt,
                add_encoder_temporal=args.add_encoder_temporal,
                add_decoder_temporal=args.add_decoder_temporal,
                condition_frames=args.temporal_patch_size,
                token_size=(args.image_size[0] * args.image_size[1])
                // ((args.downsample_size * args.patch_size) ** 2),
            )
        )
        self.vae.cuda(local_rank)
        self.vae.eval()
        print(f"load from {args.vae_ckpt}")

    @torch.no_grad()
    def encode_to_z(self, x):
        b, t, _, _, _ = x.shape
        ts = rearrange(x, "b t c h w -> (b t) c h w")
        with torch.no_grad():
            latents = self.vae.encode(ts)
        latents = patchify(latents, self.args.patch_size)
        latents = rearrange(latents, "(b t) L c -> b t L c", b=b, t=t)
        return latents

    @torch.no_grad()
    def z_to_image(self, x, is_video=False):
        x = unpatchify(x, self.args.patch_size, self.args.vae_embed_dim)
        with torch.no_grad():
            images = self.vae.decode(x, is_video)
        images = images / 2 + 0.5
        return images.clip(0, 1)

    def poses_to_indices(self, poses):
        # poses: b, F, 2
        x, y = poses[:, :, 0], poses[:, :, 1]
        internal_x, internal_y = (
            torch.floor(x * 16).clip(0, 127),
            torch.floor((y + 1) * 16).clip(0, 31),
        )
        indices = internal_x * self.latitute_bins + internal_y
        return indices.to(torch.long).unsqueeze(dim=2)
